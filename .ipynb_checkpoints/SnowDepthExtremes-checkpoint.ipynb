{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Notebook to analyse output of snow_processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import iris\n",
    "from iris.analysis import Aggregator\n",
    "import iris.plot as iplt\n",
    "import iris.quickplot as qplt\n",
    "from iris.util import rolling_window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Based on code adjusted from Scitools Iris, \n",
    "https://scitools.org.uk/iris/docs/latest/examples/General/custom_aggregation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check for consecutive exceedance of threshholds:\n",
    "#adjusted code example from https://scitools.org.uk/iris/docs/latest/examples/General/custom_aggregation.html\n",
    "\n",
    "\n",
    "# Define a function to perform the custom statistical operation.\n",
    "# Note: in order to meet the requirements of iris.analysis.Aggregator, it must\n",
    "# do the calculation over an arbitrary (given) data axis.\n",
    "def count_spells(data, threshold, axis, spell_length):\n",
    "    \"\"\"\n",
    "    Function to calculate the number of points in a sequence where the value\n",
    "    has exceeded a threshold value for at least a certain number of timepoints.\n",
    "\n",
    "    Generalised to operate on multiple time sequences arranged on a specific\n",
    "    axis of a multidimensional array.\n",
    "\n",
    "    Args:\n",
    "\n",
    "    * data (array):\n",
    "        raw data to be compared with value threshold.\n",
    "\n",
    "    * threshold (float):\n",
    "        threshold point for 'significant' datapoints.\n",
    "\n",
    "    * axis (int):\n",
    "        number of the array dimension mapping the time sequences.\n",
    "        (Can also be negative, e.g. '-1' means last dimension)\n",
    "\n",
    "    * spell_length (int):\n",
    "        number of consecutive times at which value > threshold to \"count\".\n",
    "\n",
    "    \"\"\"\n",
    "    if axis < 0:\n",
    "        # just cope with negative axis numbers\n",
    "        axis += data.ndim\n",
    "    # Threshold the data to find the 'significant' points.\n",
    "    data_hits = data > threshold\n",
    "    # Make an array with data values \"windowed\" along the time axis.\n",
    "    hit_windows = rolling_window(data_hits, window=spell_length, axis=axis)\n",
    "    # Find the windows \"full of True-s\" (along the added 'window axis').\n",
    "    full_windows = np.all(hit_windows, axis=axis+1)\n",
    "    # Count points fulfilling the condition (along the time axis).\n",
    "    spell_point_counts = np.sum(full_windows, axis=axis, dtype=int)\n",
    "    return spell_point_counts\n",
    "\n",
    "def load_data_from_netcdf (filepath):\n",
    "    # Load the whole data (as a list of cubes)\n",
    "    file_path = (filepath)\n",
    "    data = iris.load(file_path)\n",
    "    return data\n",
    "\n",
    "def get_cube_from_cubelist (data, variablename):\n",
    "    filtered_cubelist = data.extract(variablename)\n",
    "    return filtered_cubelist[0]\n",
    "    \n",
    "def calculate_data_above_threshold_for_x_days(data, threshold, numberOfDays, relativeValue):\n",
    "    \n",
    "    # Make an aggregator from the user function.\n",
    "    SPELL_COUNT = Aggregator('spell_count',\n",
    "                             count_spells,\n",
    "                             units_func=lambda units: 1)\n",
    "\n",
    "    # Calculate the statistic\n",
    "    data_above_threshold = data.collapsed('time', SPELL_COUNT,\n",
    "                                  threshold=threshold,\n",
    "                                  spell_length=numberOfDays)\n",
    "    # TODO: customize label\n",
    "    data_above_threshold.rename('Days with consecutive '+str(numberOfDays)+'-day snow falls above '+str(threshold)+'mm in timeperiod')\n",
    "    \n",
    "    # relative result\n",
    "    if(relativeValue==True):\n",
    "        data_above_threshold.data = data_above_threshold.data/18600*100\n",
    "    return data_above_threshold\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "def data_analysis(filepath, depth_thresholds,time_thresholds):\n",
    "    # retrieve data\n",
    "    data = load_data_from_netcdf(filepath)\n",
    "    print(data)\n",
    "    cube_daily = get_cube_from_cubelist(data,'approx_fresh_daily_snow_height')\n",
    "    # change and accumulation analysis to be included later, cube_change = get_cube_from_cubelist(data,'approx_change_snow_height')\n",
    "    datalist = []\n",
    "    datalist.append(cube_daily)\n",
    "    \n",
    "    \n",
    "    threshholded_data = {}\n",
    "    # loop over thresholds, calculate and plot results\n",
    "   \n",
    "    for i_data in datalist:\n",
    "        for i_time in time_thresholds:\n",
    "            for i_depth in depth_thresholds:\n",
    "                threshholded_data[i_data, i_depth, i_time] = calculate_data_above_threshold_for_x_days(i_data,i_depth,i_time, True)\n",
    "                \n",
    "    return threshholded_data # returns dictionary of data indexed by datasource and threshhold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quante/miniconda3/lib/python3.7/site-packages/iris/fileformats/cf.py:367: UserWarning: Missing CF-netCDF boundary variable 'time_bnds', referenced by netCDF variable 'time'\n",
      "  warnings.warn(message % (name, nc_var_name))\n",
      "/home/quante/miniconda3/lib/python3.7/site-packages/iris/coords.py:1410: UserWarning: Collapsing a non-contiguous coordinate. Metadata may not be fully descriptive for 'time'.\n",
      "  warnings.warn(msg.format(self.name()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: approx_accumulated_snow_height / (1) (time: 18360; latitude: 144; longitude: 192)\n",
      "1: approx_change_snow_height / (1)     (time: 18360; latitude: 144; longitude: 192)\n",
      "2: approx_fresh_daily_snow_height / (1) (time: 18360; latitude: 144; longitude: 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quante/miniconda3/lib/python3.7/site-packages/iris/coords.py:1410: UserWarning: Collapsing a non-contiguous coordinate. Metadata may not be fully descriptive for 'time'.\n",
      "  warnings.warn(msg.format(self.name()))\n"
     ]
    }
   ],
   "source": [
    "# define thresholds\n",
    "#depth thresholds\n",
    "depth_thresholds = np.arange(100,1001,100)\n",
    "# time thresholds\n",
    "time_thresholds = np.arange(1,5,1)\n",
    "# generate data\n",
    "data = data_analysis('/home/quante/projects/extremesnowevents/output_UKESM1-0-LL_ssp585_20502100.nc', depth_thresholds,time_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Some Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def plot function\n",
    "\n",
    "def contour_plot_intensity_data (data,contour_levels):  \n",
    "    # Plot the results.\n",
    "    qplt.contourf(data,contour_levels,colormap='RdBu_r')\n",
    "    plt.gca().coastlines()\n",
    "    iplt.show()\n",
    "\n",
    "\n",
    "#plot data\n",
    "contour_levels = [0.001,0.01,0.05,1,5,10]\n",
    "\n",
    "for i_keys in data.keys():\n",
    "    contour_plot_intensity_data(data[i_keys],contour_levels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
